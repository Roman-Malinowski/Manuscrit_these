\addcontentsline{toc}{chapter}{Conclusion}
\chapter*{Conclusion}
\section*{Context}
Usage of elevation data for environmental applications, urban planning, risk assessment, \etc requires the massive production of \acrfull{dsm}s. \acrshort{dsm}s at a global scale can be produced using satellites orbiting the Earth, and using different techniques such as \acrshort{lidar} measures, radar interferometry or optical photogrammetry. In particular, optical sensors, which have become relatively low-cost, allow producing dense \acrshort{dsm}s with sub-meter resolution. In this context, \acrshort{cnes} and Airbus are preparing the launch of the \acrshort{co3d} mission, consisting in two pairs of low-cost optical satellites dedicated to the production of high-resolution \acrshort{dsm}s across the globe using stereophotogrammetry.

Photogrammetry is a complex task, consisting in multiple successive algorithms to process images and extract the elevation information contained through the parallax effect. It can be divided into the following main steps:
\begin{itemize}
    \item Resampling of stereo images into a convenient geometry.
    \item Dense matching of pixels between images.
    \item Triangulation of a 3D point for each match.
    \item Rasterization of the point cloud on a regular grid.
\end{itemize}
Along these steps, many uncertainties arise, which can lead to errors of varying magnitude. The objective of this thesis was to quantify and propagate the uncertainty alongside a photogrammetry pipeline, in preparation of the \acrshort{co3d} mission. In particular, we focused on the \acrshort{cars} pipeline which will be used to process the \acrshort{co3d} data. Furthermore, one of the requirements of the \acrshort{co3d} mission is to produce a performance map alongside \acrshort{dsm}s. Many \acrshort{dsm} users also seek to know the quality of the predicted \acrshort{dsm}, usually characterized by confidence intervals. The main contribution of this thesis is precisely the development of a method for computing confidence intervals, which can also be used as a performance map for the \acrshort{co3d} mission.

\section*{Contributions}
In this thesis, we used other uncertainty models than well-known probability distributions. We considered \textit{imprecise} probabilities and more specifically possibility distributions, more adapted to model epistemic uncertainty, \ie arising from a lack of knowledge, by opposition to the uncertainty due to a purely random process. Those models define credal sets, \ie convex sets of probability distributions. 

When propagating multiple sources of uncertainty, it is required to compute a multivariate model of uncertainty accounting for the different dependencies between uncertain sources. We proposed to use dependency models known as copulas to construct multivariate credal sets. We introduced three methods for aggregating marginal credal sets into multivariate credal sets using copulas, namely $\M_{robust}$, $\M_{mass}$ and $\M_{agg}$. Those models are not equivalent, and their computation presents varying degrees of complexity. We investigated the relations between those sets depending on the copula used to join them, as well as the specific models used to define marginal credal sets, such as the aforementioned possibility distributions.

We used the previous results concerning multivariate credal sets to propagate the uncertainty in a specific part of the photogrammetry pipeline: the dense matching step. More specifically, we consider the computation of the matching cost between every pixel of the stereo images. We showed that the models could correctly estimate the propagated uncertainty regarding the matching cost on a real pair of stereo images.

The cost volume is used to compute the disparity map, encoding the pairing of pixels to be triangulated. Computing a cost volume allowing to correctly estimate the disparity is the hardest part of the stereo pipeline. Correctly estimating the uncertainty on the predicted disparity is therefore crucial for the rest of the pipeline. However, only considering the propagated uncertainty on the cost volume, as we previously did, is not sufficient for the correct uncertainty estimation of the disparity map. We therefore proposed to use possibility distributions to model the epistemic uncertainty regarding the choice of each disparity. For each considered pixel, we used those possibility distributions to determine a disparity confidence interval. We evaluated the accuracy of the intervals using real stereo images, and observed that intervals contain the true disparity at least $90\%$ of the time. This method for creating intervals can be applied to a wide range of stereo algorithms and is not restricted to satellite photogrammetry. To the best of our knowledge, it is also the first time such disparity confidence intervals are computed. 

We then propagated those disparity confidence intervals all the way to the end of the pipeline, where they take the form of elevation confidence intervals associated with the predicted \acrshort{dsm}. We evaluated the performance of elevation intervals on real satellite images, for which we possess a reference high quality \acrshort{dsm}. Intervals are once again accurate $90\%$ of the time, validating the performances of our new method. We also implemented this method for estimating disparity and elevation confidence intervals into the \acrshort{cars} (\url{https://github.com/CNES/cars}) and Pandora (\url{https://github.com/CNES/Pandora}) software, developed by \acrshort{cnes}. They are already publicly available, and can be used to produce elevation confidence intervals for \acrshort{dsm}s.

\section*{Limitations and Perspectives}
We demonstrated that possibility distributions and copulas could be used to propagate uncertainty in a problem such as the evaluation of a cost volume. Implementing this propagation remains a difficult challenge. It requires using simple cost functions and other simplifying assumptions. It also requires large processing capacities to be carried out efficiently, as we joined the uncertainty of thousands of different pixels in our experiments.

We evaluated our method for producing elevation intervals using high resolution \acrshort{dsm}s obtained from \acrshort{lidar} data. However, we only had access to \acrshort{dsm}s provided by glaciologists, or high resolution point clouds from the \acrshort{lidar} HD program. This means that we only observed landscapes which either contained glaciers, or were located in France. Extending the evaluation of intervals to a broader diversity of locations would be valuable, such as deserts or American cities.

Another limitation of our method is that it does not take into consideration the potential errors occurring \textit{before} the dense matching step. Those errors could occur when defining the epipolar geometry for instance, or on the localization model itself, for instance caused by vibration of the satellite as seen at the end of \Cref{chap:elevation_intervals}. Combining uncertainty models of the sensor itself or on its geolocation model could lead to a better estimation of the overall uncertainty of the \acrshort{dsm}. 

Different future perspective regarding our work can be considered. The method we developed for computing confidence intervals is carried out alongside the processing of the main \acrshort{dsm}, without influencing the values it contains. However, we could imagine using the information contained in the confidence intervals to facilitate or improve the different disparity or elevation predictions. Here are a few interesting leads that could be explored:
\begin{itemize}
    \item Disparity intervals could be computed a first time before any \acrshort{sgm} regularization. Then, during the \acrshort{sgm} regularization, disparities that are not contained inside disparity intervals are ignored, which could greatly reduce the amount of computation necessary. 
    \item Another approach would be to use matching cost possibilities to apply a different strategy when computing the disparity map. Currently, disparities are determined using a \textit{winner-takes-all} strategy, meaning that for each pixel, the selected disparity is the one minimizing its cost curve. We then do the same for the cost volume of the right image, and remove matches that do not verify the cross-checking test of \Cref{eq:cross-checking}. However, we could see the choice of a disparity map as a stable marriage problem \cite{irving_matching_1998}, where possibilities are interpreted as degrees of preferences. This could improve the quality of the disparity map, as the choice of each disparity would consider more information than a single cost curve. It would also provide an alternative to the \textit{winner takes all} strategy, which accounts for the vast majority of strategies used in dense matching.  
    \item We currently extend intervals in low confidence areas using quantiles computed over a set of neighboring intervals. We could consider using instead other cost functions which present better performances near depth discontinuities, or using values of the cost volume before \acrshort{sgm} regularization to better process intervals in those low confidence areas.
    \item When triangulating disparity intervals, we could take into consideration the limited resolution of line of sights. We could for instance reason with uncertain 3D volumes when computing their intersection, which would be a more realistic model than the precise lines of sight currently considered.
    \item Before the rasterization step, we could discard points for which the interval size is too large, as the potential error committed by considering this point is too high. 
    \item During rasterization, we could use information from intervals to modify the weights of each point in the final value of the \acrshort{dsm}. Points with small elevation intervals would be granted more importance in the final product than those with large intervals. 
\end{itemize}

Ideas developed in this thesis for 1D matching could be extended to 2D matching, for instance in the Pandora2D tool (\url{https://github.com/CNES/Pandora2D}). Apart from photogrammetry, ideas developed in this thesis could be used to improve confidence criteria of the alignment of image bands for multi-spectral images, for instance in the TRISHNA mission \cite{lagouarde_indo-french_2019}, or alignment of multi-temporal images, such as Sentinel-2 \cite{yan_sentinel-2a_2018}.
Taking a step back from imagery, we showed in this thesis that using other models of uncertainty than well-known probabilities can lead to new methods for estimating and characterizing potential errors. Many methods using incomplete or imperfect data for diverse applications, such as clustering, classification or decision-making, can also benefit from using imprecise probabilities.

\clearpage