\addcontentsline{toc}{chapter}{Conclusion}
\chapter*{Conclusion}
\section*{Context}
Usage of elevation data for environmental stakes, urban planning, risk assessment, \etc necessitates the massive production of \acrlong{dsm}s. Producing \acrshort{dsm}s at a global scale can be done using satellites orbiting the Earth, and using different techniques such as \acrshort{lidar} measures, radar interferometry of\comloic{c'est of ou or?} optical photogrammetry. Optical sensors in particular, which have become relatively low cost, allow to produce dense \acrshort{dsm} with sub-meter resolution. In this context, \acrshort{cnes}\comloic{alors d'habitude c'est CNES et Airbus, à voir} is preparing the launch of the \acrshort{co3d} mission, consisting in two pairs of low cost optical satellites dedicated to the production of high-resolution \acrshort{dsm} across the globe using stereophotogrammetry.

Photogrammetry is a complex task, consisting in multiple successive algorithms to process the images and extract the elevation information they contain through the parallax effect. It can be divided into the following main steps:
\begin{itemize}
    \item Resampling of stereo images in\comloic{into?} a convenient geometry.
    \item Dense matching of pixels between images.
    \item Triangulation of a 3D point for each match.
    \item Rasterization of the point cloud on a regular grid.
\end{itemize}
Along those steps, many uncertainties arise, which can then \comloic{peut être que le then est de trop}lead to errors of varying magnitude. The objective of this thesis was to quantify and propagate the uncertainty alongside a photogrammetry pipeline, in preparation of the \acrshort{co3d} mission. In particular, we focused on the \acrshort{cars} pipeline which will be used to process the \acrshort{co3d} data. Furthermore, one of the requirements of the \acrshort{co3d} mission is to produce performance map alongside \acrshort{dsm}s. Many \acrshort{dsm} users also seek to know the quality of the predicted \acrshort{dsm}, usually characterized by confidence intervals. The main contribution of this thesis is precisely the development of a method to compute confidence intervals, which can also be used as a performance map for the \acrshort{co3d} mission.

\section*{Models of Uncertainty}\commanue{alors moi j'ai toujours un pb avec les temps mais c'est aussi vrai en français. Donc là j'aurais tendance à mettre au passé mais ce n'est que mon avis}
In this thesis, we use other uncertainty models than well-known probability distributions. We consider \textit{imprecise} probabilities and more specifically possibility distributions, more adapted to model epistemic uncertainty, \ie arising from a lack of knowledge, by opposition to the uncertainty due to a purely random process. Those models define credal sets, \ie convex sets of probability distributions. 

When propagating multiple sources of uncertainty, it is required to compute a multivariate model of uncertainty accounting for the different dependencies between uncertain sources. We propose to use dependency models known as copulas to construct multivariate credal sets. We introduce three different methods for aggregating marginal credal sets into multivariate credal sets using copulas, namely $\M_{robust}$, $\M_{mass}$ and $\M_{agg}$. Those models are not equivalent, and their computation presents varying degrees of complexity. We investigate the relations between those sets depending on the copula used to join them, and for marginal credal sets defined by specific models such as the aforementioned possibility distribution.

We use the previous results concerning multivariate credal sets to propagate the uncertainty in a specific part of the photogrammetry pipeline: the dense matching step. More specifically, we consider the computation of the matching cost between every pixels of the stereo images. We showed\commanue{ah là tu es au passé!} that the models could correctly estimate the propagated uncertainty regarding the matching cost on a real pair of stereo images.

The cost volume is used to compute disparity map, encoding the pairing of pixels to be triangulated. Computing a cost volume allowing to correctly estimate the disparity is the hardest part of the stereo pipeline. Correctly estimating the uncertainty on the predicted disparity is therefore crucial for the rest of the pipeline. Only considering the propagated uncertainty on the cost volume, as we previously did, is not sufficient for a correct uncertainty estimation of the disparity map. We therefore propose to use possibility distributions to model the epistemic uncertainty regarding the choice of each disparity. For each considered pixel, we use those possibility distributions to determine a disparity confidence interval. We evaluate the accuracy of the intervals using real stereo images, and observe that intervals contain the true disparity at least $90\%$ of the time. This method for creating intervals can be applied to a wide range of stereo algorithms and is not restricted to satellite photogrammetry. To our knowledge, it is also the first time such disparity confidence intervals are computed. 

We then propagate those disparity confidence intervals all the way to the end of the pipeline, where they take the form of elevation confidence intervals associated to the predicted \acrshort{dsm}. We evaluate the performances of the elevation intervals on real satellite images, for which we possess a reference high quality \acrshort{dsm}. The intervals are once again accurate $90\%$ of the time, validating the performances of our new method.

\section*{Limitations and Perspectives}
We demonstrated that possibility distributions and copulas could be used to propagate uncertainty in a problem such as the evaluation of a cost volume, although implementing this propagation remains a difficult challenge. It requires to use simple cost functions and other simplifying assumptions. It also necessitates good processing capacities to be carried out efficiently, as we joined the uncertainty of thousands of different pixels in our experiments.

We evaluated our method for producing elevation intervals using high resolution \acrshort{dsm} obtained from \acrshort{lidar}. However, we only had access to \acrshort{dsm} provided glaciologists\comloic{provided by?}, or high resolution point clouds from the \acrshort{lidar} HD program. This means that we only observed landscape which either contained glaciers, or located in France. Extending the evaluation of intervals to a broader diversity of locations would be valuable, as we cannot guarantee with certainty that our method would also have good performances on deserts or American cities for instance. 

Another limitation of our method is that it does not take into consideration the potential errors occurring \textit{before} the dense matching step. Those errors could occur when defining the epipolar geometry for instance or on the localisation model itself, as caused by vibration of the satellite as seen at the end of \Cref{chap:elevation_intervals}. 

Different perspective following our work can be considered. The method we developed for computing confidence intervals (would they be expressed in disparity or in elevation) is carried out alongside the processing of the main \acrshort{dsm}, without influencing the values it contains. We could however imagine to use the information contained in the confidence intervals to facilitate or improve the different disparity or elevation predictions. Here are a few interesting leads that could be explored:
\begin{itemize}
    \item Disparity intervals could be computed a first time before any \acrshort{sgm} regularization. Then, during the \acrshort{sgm} regularization, disparities that are not contained inside disparity intervals are ignored, which could greatly reduce the amount of computation necessary. 
    \item Another idea would be to use matching cost possibilities to apply a different strategy when computing the disparity map. Currently, disparities are determined using a \textit{winner takes all} strategy, meaning that for each pixel, the selected disparity is the one minimizing its cost curve. We then do the same for the cost volume of the right image, and remove matches that do not verify the cross-checking test of \cref{eq:cross-checking}. However, we could see the choice of a disparity map as a stable marriage problem \cite{irving_matching_1998}, where possibilities are interpreted as degrees of preferences. This could improve the quality of the disparity map, as the choice of each disparity would consider more information than a single cost curve. It would also provide an alternative to the \textit{winner takes all} strategy, which makes up for the vast majority of strategies used in dense matching.  
    \item When triangulating disparity intervals, we could take into consideration the limited resolution of line of sights. We could for instance reason with uncertain 3D volumes when computing their intersection, which would be a more realistic model than the precise lines of sight currently considered.
    \item Before the rasterization step, we could filter out\commanue{discard?} points for which the interval size is too large, as the potential error committed by considering this point is too high. 
    \item During the rasterization process, we could use information from intervals to modify the weights of each point in the final value of the \acrshort{dsm}. Points with small elevation intervals would be granted more importance in the final product than those with large intervals. 
\end{itemize}

\youshallnotpass
\pagebreak